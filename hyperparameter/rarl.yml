# AIGC START
# PPO 版本的 RARL 配置（移除了 TRPO 特有的参数）
BipedalWalker-v3:
  normalize: true
  n_envs: 16
  device: cuda:0
  n_timesteps: !!float 5e6
  N_mu: !!float 10
  N_nu: !!float 10
  protagonist_policy: "MlpPolicy"
  adversary_policy: "MlpPolicy"
  n_steps_protagonist: 2048
  n_steps_adversary: 128
  adv_fraction: 1.0
  protagonist_kwargs: "dict(batch_size=64,
                            gamma=0.99,
                            learning_rate=2.5e-4,
                            gae_lambda=0.95,
                            n_epochs=10,
                            clip_range=0.2
                            )"
  protagonist_policy_kwargs: "dict(
                              activation_fn=torch.nn.Tanh,
                              net_arch=[dict(pi=[64, 64], vf=[64, 64])]
                            )"
  adversary_kwargs: "dict(batch_size=64,
                            gamma=0.99,
                            learning_rate=2.5e-4,
                            gae_lambda=0.95,
                            n_epochs=10,
                            clip_range=0.2
                            )"
  adversary_policy_kwargs: "dict(
                            activation_fn=torch.nn.Tanh,
                            net_arch=[dict(pi=[64, 64], vf=[64, 64])]
                          )"
# AIGC END

# AIGC START
# PPO 版本的 LunarLanderContinuous-v2 配置
LunarLanderContinuous-v2:
  n_envs: 16
  n_timesteps: !!float 50
  N_mu: !!float 10
  N_nu: !!float 10
  protagonist_policy: "MlpPolicy"
  adversary_policy: "MlpPolicy"
  n_steps_protagonist: 128
  n_steps_adversary: 128
  protagonist_kwargs: "dict(batch_size=128,
                            gamma=0.999,
                            learning_rate=0.0078,
                            gae_lambda=0.92,
                            n_epochs=10,
                            clip_range=0.2
                            )"
  protagonist_policy_kwargs: "dict(activation_fn=torch.nn.ReLU,
                              net_arch=[dict(pi=[64, 64], vf=[64, 64])]
                              )"
  adversary_kwargs: "dict(batch_size=128,
                            gamma=0.999,
                            learning_rate=0.0078,
                            gae_lambda=0.92,
                            n_epochs=10,
                            clip_range=0.2
                            )"
  adversary_policy_kwargs: "dict(activation_fn=torch.nn.ReLU,
                              net_arch=[dict(pi=[64, 64], vf=[64, 64])]
                              )"
# AIGC END


# AIGC START
# PPO 版本的 HalfCheetah-v3 配置（移除了 TRPO 特有的参数）
HalfCheetah-v3:
  normalize: true
  n_envs: 1
  device: cuda:0
  n_timesteps: !!float 50
  N_mu: !!float 5
  N_nu: !!float 1
  protagonist_policy: "MlpPolicy"
  adversary_policy: "MlpPolicy"
  n_steps_protagonist: 2048
  n_steps_adversary: 128
  adv_fraction: 2.5
  protagonist_kwargs: "dict(batch_size=128,
                            gamma=0.95,
                            learning_rate=1.37e-05,
                            gae_lambda=0.9,
                            n_epochs=10,
                            clip_range=0.2
                            )"
  protagonist_policy_kwargs: "dict(
                              activation_fn=torch.nn.ReLU,
                              net_arch=[dict(pi=[256, 256], vf=[256, 256])]
                            )"

  adversary_kwargs: "dict(batch_size=512,
                            gamma=0.999,
                            learning_rate=0.0225,
                            gae_lambda=0.92,
                            n_epochs=10,
                            clip_range=0.2
                            )"
  adversary_policy_kwargs: "dict(
                            activation_fn=torch.nn.ReLU,
                            net_arch=[dict(pi=[64, 64], vf=[64, 64])]
                          )"
# AIGC END

#  normalize: true
#  n_envs: 1
#  device: cuda:1
#  n_timesteps: !!float 500
#  N_mu: !!float 1
#  N_nu: !!float 1
#  protagonist_policy: "MlpPolicy"
#  adversary_policy: "MlpPolicy"
#  n_steps_protagonist: 2048
#  n_steps_adversary: 2048
#  adv_fraction: 1.0
#  protagonist_kwargs: "dict(batch_size=128,
#                            gamma=0.99,
#                            learning_rate=1.9904955094948545e-05,
#                            n_critic_updates=25,
#                            cg_max_steps=30,
#                            target_kl=0.01,
#                            gae_lambda=0.95
#                            )"
#  protagonist_policy_kwargs: "dict(
#                              activation_fn=torch.nn.ReLU,
#                              net_arch=[dict(pi=[64, 64], vf=[64, 64])]
#                            )"
#
#  adversary_kwargs: "dict(batch_size=128,
#                            gamma=0.99,
#                            learning_rate=1.9904955094948545e-05,
#                            n_critic_updates=25,
#                            cg_max_steps=30,
#                            target_kl=0.01,
#                            gae_lambda=0.95
#                            )"
#  adversary_policy_kwargs: "dict(
#                            activation_fn=torch.nn.ReLU,
#                            net_arch=[dict(pi=[64, 64], vf=[64, 64])]
#                          )"

# specified by paper
# n_envs: 1
# n_timesteps: !!float 500 (100 for Inverted Pendulum)
# adversary_policy_kwargs: "dict(
#                            activation_fn=torch.nn.ReLU,
#                            net_arch=[64. 64]
#                          )"